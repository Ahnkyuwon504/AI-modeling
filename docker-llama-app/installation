####################################
## 필요한 것
####################################
# AWS 계정
# huggingface token id(READ)
# https://huggingface.co/google/gemma-2-2b-it 모델에 대한 권한

####################################
## 인스턴스 시작
####################################
# 패키지 리스트 업데이트
$ sudo apt-get update

# 필수 패키지 설치
$ sudo apt-get install \
	apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
    software-properties-common

####################################
## docker 설치
####################################    
# GPG key 추가
$ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc

# docker 저장소 설정
$ echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 패키지 리스트 다시 업데이트
$ sudo apt-get update

# docker 설치
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# 현재 사용자를 docker그룹에 추가
$ sudo usermod -aG docker $USER

# 도커세션 새로고침
$ newgrp docker

# 확인
$ docker run hello-world

####################################
## git 설치
####################################
$ sudo apt install git

# 확인
$ git --version

# ssh키 추가
$ ssh-keygen -t rsa -b 2048 -f ~/.ssh/test_key

# config
$ vi ~/.ssh/config

# config
Host github.com
     HostName github.com
     User git
     IdentityFile ~/.ssh/test_key
     IdentitiesOnly yes
     Port 22

# git clone

####################################
## python 설치
####################################
# Python 3 설치
$ sudo apt install python3

# 확인
$ python3 --version

# pip 설치
$ sudo apt install python3-pip

# 가상환경 설치
$ sudo apt install python3-venv

# 가상환경 생성
$ python3 -m venv kyuenv

####################################
## llama.cpp 설치
####################################
$ git clone https://github.com/ggerganov/llama.cpp
$ cd llama.cpp

# 빌드
$ make -j

####################################
## CPU 기반 추론
####################################

####################################
## 환경변수 설정
####################################
$ echo 'export HF_TOKEN="{token id}"' >> ~/.bashrc
$ source ~/.bashrc

####################################
## 필요 패키지 설치
####################################
$ source kyuenv/bin/activate
$ pip install -r requirements.txt

####################################
## 모델 다운로드
####################################
$ python3 download.py

####################################
## 모델 변환(safetensors -> bf16 gguf)
####################################
$ sh convert.sh

# gguf제외하고 전부 삭제
$ find . -type f ! -name '*.gguf' -delete

####################################
## 양자화(bf16 gguf -> 5bit)
####################################
$ sh quantize.sh

####################################
## 추론(16비트/5비트)
####################################
$ sh llama.cpp.gguf.infer.sh

# gemma2 16bit 기준 약 60초
# gemma2 5bit 기준 약 35초

####################################
## GPU 기반 추론
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## 
####################################

####################################
## TODO
####################################
# historical/multi-turn
# token streaming