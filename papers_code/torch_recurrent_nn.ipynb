{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ2qBhpu6oBoalXoHTJn4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahnkyuwon504/AI-modeling/blob/main/papers_code/torch_recurrent_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import modules"
      ],
      "metadata": {
        "id": "GhKKhcWYTzKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GU07Urf8Tvqq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터셋"
      ],
      "metadata": {
        "id": "4YTvy6dO5FVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dtype = torch.float\n",
        "- 텐서의 데이터 타입 지정\n",
        "- `torch.float` PyTorch에서 부동 소수점 수(32비트 부동 소수점, torch.float32)"
      ],
      "metadata": {
        "id": "uhxnJsxSc8-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
        "dtype = torch.float"
      ],
      "metadata": {
        "id": "Ug5H0HQa2Njy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 단어 처리"
      ],
      "metadata": {
        "id": "8YcXxDkI2MU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_list\n",
        "- sentence를 \" \" 두고 하나로 결합\n",
        "- 공백 기준으로 split\n",
        "- 중복제거한 집합생성 `Java의 HashSet`\n",
        "- 다시 list로 변환\n",
        "\n",
        "word_dict\n",
        "- word_dict: {'dog': 0, 'i': 1, 'cat': 2}\n",
        "- number_dict: {0: 'dog', 1: 'i', 2: 'cat'}"
      ],
      "metadata": {
        "id": "vTEEYw47ddX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "n_class = len(word_dict)"
      ],
      "metadata": {
        "id": "rZeivvqN2Mc-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. RNN모델 하이퍼파라미터"
      ],
      "metadata": {
        "id": "i0hDfwZj2OF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size\n",
        "- 한 번의 에포크에서 처리할 문장의 개수\n",
        "\n",
        "n_step\n",
        "- RNN이 한 번에 처리할 입력 시퀀스의 길이\n",
        "- 각 문장에서 마지막 단어를 예측하기 위해 그 앞의 두 단어 사용\n",
        "- 현재 입력되는 문장이 단어 3개로 구성되어 있으므로 3-1 = 2\n",
        "\n",
        "n_hidden\n",
        "- 은닉층의 뉴런 수"
      ],
      "metadata": {
        "id": "DQob3hDFfU0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = len(sentences)\n",
        "n_step = 2  # 학습 하려고 하는 문장의 길이 - 1\n",
        "n_hidden = 5  # 은닉층 사이즈"
      ],
      "metadata": {
        "id": "XQSCbDVO2OF1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 배치데이터 생성"
      ],
      "metadata": {
        "id": "NKWX1x_T2OLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_HR58pDjDXE",
        "outputId": "6b7ac485-7b11-4746-bf48-a897ebae670f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cat': 0,\n",
              " 'like': 1,\n",
              " 'you': 2,\n",
              " 'coffee': 3,\n",
              " 'hate': 4,\n",
              " 'dog': 5,\n",
              " 'love': 6,\n",
              " 'i': 7,\n",
              " 'milk': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr = [word_dict[n] for n in sentences[1].split()[:-1]]\n",
        "\n",
        "print(np.eye(n_class)[curr])\n",
        "print(word_dict[sentences[1].split()[-1]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK0kaVxdh94d",
        "outputId": "3b5df937-4aa0-4bf4-dc0b-83588c319d73"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(sentences):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for sen in sentences:\n",
        "    # sen = 'i like dog'\n",
        "    # word = ['i', 'like', 'dog']\n",
        "    # word[:-1] = ['i', 'like']\n",
        "    word = sen.split()\n",
        "\n",
        "    # i 에 해당하는 index 7, like 에 해당하는 index 1\n",
        "    # [7, 1]\n",
        "    input = [word_dict[n] for n in word[:-1]]\n",
        "    # dog 에 해당하는 index 5\n",
        "    # [5]\n",
        "    target = word_dict[word[-1]]\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    input_batch.append(np.eye(n_class)[input])\n",
        "    target_batch.append(target)\n",
        "\n",
        "  return input_batch, target_batch\n",
        "\n",
        "'''\n",
        "input_batch[1]\n",
        "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
        " [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
        "\n",
        " target_batch[1]\n",
        " 3\n",
        "'''\n",
        "input_batch, target_batch = make_batch(sentences)\n",
        "input_batch = torch.tensor(input_batch, dtype=torch.float32)\n",
        "# input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
        "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "0Sya2M3f2OLm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 모델링"
      ],
      "metadata": {
        "id": "MfSryuHr25Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextRNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    # nn.Module의 초기화 메서드 호출\n",
        "    # TextRNN 클래스가 nn.Module의 기능을 상속\n",
        "    # custom NN구조 정의\n",
        "    super(TextRNN, self).__init__()\n",
        "\n",
        "    # input_size는 입력단어의 개수. 원핫인코딩 차원\n",
        "    # 뉴런을 30% 무작위 비활성화\n",
        "    # input -> hidden layer / hidden layer -> hidden layer 가중치 관리\n",
        "    self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
        "\n",
        "    # hidden layer -> output layer 가중치 관리\n",
        "    self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
        "    self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
        "    self.Softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  # 순전파\n",
        "  def forward(self, hidden, X):\n",
        "    # 일반적으로 (시퀀스 길이, 배치 크기, 피쳐 수) 형태의 입력\n",
        "    # 그러나 데이터 준비 과정에서는 입력이 (배치 크기, 시퀀스 길이, 피쳐 수) 형태로 제공\n",
        "    # 따라서 RNN에 입력하기 전에 텐서의 차원을 교환\n",
        "    X = X.transpose(0, 1)\n",
        "    outputs, hidden = self.rnn(X, hidden)\n",
        "    outputs = outputs[-1]  # 최종 예측 Hidden Layer\n",
        "    # torch.mm은 행렬 곱셈 수행\n",
        "    # 최종 은닉 상태 outputs과 가중치 행렬 self.W를 곱하고, 편향 self.b를 더해 최종 출력값을 계산\n",
        "    model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n",
        "    return model"
      ],
      "metadata": {
        "id": "MKA5wfTv25Hq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 학습"
      ],
      "metadata": {
        "id": "_cKIB4U-25sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextRNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "  # 가중치 초기화가 아니라 은닉상태를 초기화하는 것\n",
        "  hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
        "  output = model(hidden, input_batch)\n",
        "  loss = criterion(output, target_batch)\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print(output)\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni5kwv-525s4",
        "outputId": "42eebb01-283e-4037-dd61-2ee9a7a36b8b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.4540, -0.9499,  0.4646, -0.6645, -0.8106,  4.4832, -2.3059, -3.0883,\n",
            "          0.6635],\n",
            "        [-1.2431, -2.8590, -2.4866,  1.4203, -2.6205, -1.8600, -2.5311, -1.1958,\n",
            "          1.1660],\n",
            "        [-1.5217, -2.6091, -2.4897,  1.3455, -2.3233, -0.7837, -2.5932, -1.3597,\n",
            "          1.2142],\n",
            "        [ 2.9414, -2.4777,  0.0978, -0.1847, -2.8857, -0.7146, -2.4623, -3.1710,\n",
            "          0.8587],\n",
            "        [-0.2505, -3.6807, -2.3999,  1.5084, -3.6936, -4.5352, -2.5290, -1.3570,\n",
            "          1.2925],\n",
            "        [-0.1997, -3.6387, -2.4835,  1.5290, -3.7252, -4.3112, -2.6384, -1.4968,\n",
            "          1.3141]], grad_fn=<AddBackward0>)\n",
            "Epoch: 0100 cost = 0.608090\n",
            "tensor([[ 1.6242, -1.1194, -0.1992, -1.2140,  0.1391,  6.0031, -2.5098, -1.6199,\n",
            "          1.1518],\n",
            "        [ 0.3779, -4.3031, -3.1280,  2.8941, -2.2058, -3.0328, -3.3579,  0.3576,\n",
            "          1.3267],\n",
            "        [-3.2712, -2.9141, -3.3150,  1.1769, -2.1700, -0.3117, -3.1829, -1.5669,\n",
            "          2.7915],\n",
            "        [ 4.1994, -2.5368,  0.0124, -0.2119, -3.1258, -0.8503, -2.7916, -4.3320,\n",
            "          0.9258],\n",
            "        [ 0.0901, -4.7052, -2.6473,  1.9425, -4.3911, -5.5608, -3.3506, -2.9773,\n",
            "          2.5613],\n",
            "        [-2.4162, -3.6990, -3.7043,  2.8557, -3.9592, -6.1321, -3.1076, -1.8498,\n",
            "          1.8379]], grad_fn=<AddBackward0>)\n",
            "Epoch: 0200 cost = 0.246944\n",
            "tensor([[ 1.8105, -0.8836, -0.2678, -1.3120,  0.8353,  7.3075, -2.3654, -0.5266,\n",
            "          1.2697],\n",
            "        [-0.0122, -4.9533, -3.3203,  4.2686, -2.5803, -2.8734, -3.7535, -0.4152,\n",
            "          1.7042],\n",
            "        [-2.2778, -3.6272, -3.0665,  0.3161, -2.6581, -0.3277, -3.6448, -2.6450,\n",
            "          4.4732],\n",
            "        [ 4.7656, -2.4642, -0.0447, -0.6949, -3.1287, -1.1292, -2.7900, -4.8574,\n",
            "          1.2869],\n",
            "        [ 0.5759, -5.3754, -2.3222,  1.4823, -4.7693, -6.2563, -3.5402, -4.1198,\n",
            "          3.8096],\n",
            "        [-3.9461, -3.2878, -4.1122,  4.0996, -3.7031, -7.5044, -2.9808, -1.5746,\n",
            "          1.0824]], grad_fn=<AddBackward0>)\n",
            "Epoch: 0300 cost = 0.061019\n",
            "tensor([[ 1.9678, -0.7216, -0.2732, -1.2680,  0.9952,  8.0620, -2.2873, -0.2808,\n",
            "          1.1285],\n",
            "        [-0.4193, -5.0938, -3.5312,  5.0105, -2.7280, -2.7810, -3.8650, -0.6463,\n",
            "          1.7158],\n",
            "        [-2.0095, -3.8824, -3.0665,  0.0246, -2.8858, -0.3796, -3.8286, -3.0552,\n",
            "          5.2149],\n",
            "        [ 5.3099, -2.5146, -0.1061, -0.9693, -3.1001, -1.4530, -2.7055, -4.8749,\n",
            "          1.3695],\n",
            "        [ 0.4739, -5.5508, -1.7712,  0.9042, -4.5645, -7.0504, -3.0465, -3.8600,\n",
            "          4.2057],\n",
            "        [-4.2448, -3.2127, -4.1718,  4.6761, -3.5563, -8.1655, -2.8913, -1.4006,\n",
            "          0.6436]], grad_fn=<AddBackward0>)\n",
            "Epoch: 0400 cost = 0.028717\n",
            "tensor([[ 2.1351, -0.6260, -0.2783, -1.2309,  1.0541,  8.5636, -2.2425, -0.1766,\n",
            "          1.0197],\n",
            "        [-0.5901, -5.1746, -3.7166,  5.5548, -2.8178, -2.6880, -3.9667, -0.7769,\n",
            "          1.6693],\n",
            "        [-1.9372, -3.9829, -3.1235, -0.1134, -2.9941, -0.3660, -3.9396, -3.2597,\n",
            "          5.6555],\n",
            "        [ 5.7437, -2.5683, -0.1766, -1.0891, -3.0825, -1.6822, -2.6669, -4.8586,\n",
            "          1.3770],\n",
            "        [ 0.2694, -5.6357, -1.3851,  0.4448, -4.3614, -7.6422, -2.6668, -3.5553,\n",
            "          4.4794],\n",
            "        [-4.2857, -3.2350, -4.2353,  5.0674, -3.5102, -8.5737, -2.9022, -1.3779,\n",
            "          0.4121]], grad_fn=<AddBackward0>)\n",
            "Epoch: 0500 cost = 0.016696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. 예측"
      ],
      "metadata": {
        "id": "5BmiDS3-2558"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KB2sLbu0T0BS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b83d0e-a376-49d0-b24d-f6701f892bd6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dog', 'coffee', 'milk', 'cat', 'milk', 'coffee']\n"
          ]
        }
      ],
      "source": [
        "# 예측을 위해 다시 은닉상태 초기화\n",
        "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
        "\n",
        "# data는 텐서의 데이터\n",
        "# max는 1행에서의 최댓값을 차원 유지하며 가져옴\n",
        "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
        "\n",
        "# sqeeuze 텐서에서 차원이 1인 부분을 제거\n",
        "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
      ]
    }
  ]
}