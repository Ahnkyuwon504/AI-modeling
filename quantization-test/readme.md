## [Quantization]

`양자화(Quantization]` 언어모델의 매개변수를 실수형 변수에서 정수형 변수로 변환하는 과정을 통해 더 작은 비트로 변환하는 과정을 거쳐 실제 사이즈보다 작은 모델로 로드

- 비트수를 낮춰가면 필연적으로 성능이 떨어지지만, 용량과 메모리 필요량이 줄어들게 되고, 연산량도 낮아져서 token 생성 속도가 빨라짐
- 현재 1비트 양자화까지 나와 있는 상태
- 16비트 모델과 유사한 성능은 4비트까지가 한계
- 해당 Repo에서는 transformer 라이브러리의 BitAndBytes 모듈 사용
